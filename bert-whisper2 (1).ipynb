{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":97919,"databundleVersionId":11694977,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install git+https://github.com/openai/whisper.git","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport whisper\nimport os\nimport numpy as np\nimport pandas as pd\nfrom transformers import BertTokenizer, BertModel, BertForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm import tqdm_notebook as tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Whisper Setup (Simple Whisper Library)\nwhisper_model_name = \"base\"\nwhisper_model = whisper.load_model(whisper_model_name).to(\"cuda\")\n\n# BERT Setup\nbert_model_name = \"bert-base-uncased\"\nbert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\nbert_model = BertModel.from_pretrained(bert_model_name).to(\"cuda\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mlp_head = torch.nn.Sequential(\n    torch.nn.Linear(768, 256),\n    torch.nn.ReLU(),\n    torch.nn.Linear(256, 1),\n    torch.nn.Sigmoid()\n).to(\"cuda\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def transcribe_audio(audio_file_path):\n    \"\"\"Transcribes audio using Whisper (Simple Whisper Library).\"\"\"\n    try:\n        result = whisper_model.transcribe(audio_file_path)\n        return result[\"text\"]\n    except Exception as e:\n        print(f\"Error transcribing {audio_file_path}: {e}\")\n        return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_bert_embeddings(texts, max_length=128, batch_size=32):\n    \"\"\"Generates BERT embeddings in batches.\n\n    Args:\n        texts (list of str): List of text strings to generate embeddings for.\n        bert_model (BertModel): The pre-trained BertModel.\n        bert_tokenizer (BertTokenizer): The BertTokenizer.\n        max_length (int): Maximum sequence length.\n        batch_size (int): Batch size.\n\n    Returns:\n        torch.Tensor: Tensor containing the BERT embeddings.\n    \"\"\"\n    try:\n        inputs = bert_tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_length)\n        dataset = TensorDataset(inputs.input_ids, inputs.attention_mask)\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n        all_embeddings = []\n        bert_model.eval()\n        with torch.no_grad():\n            for batch in dataloader:\n                input_ids_batch, attention_mask_batch = batch\n                input_ids_batch = input_ids_batch.to(\"cuda\")\n                attention_mask_batch = attention_mask_batch.to(\"cuda\")\n                outputs = bert_model(input_ids_batch, attention_mask=attention_mask_batch)\n                embeddings = outputs.last_hidden_state[:, 0, :]  # Take the [CLS] token embedding\n                all_embeddings.extend(embeddings.cpu().numpy())\n        return torch.tensor(all_embeddings).to(\"cuda\")\n    except Exception as e:\n        print(f\"Error processing text: {e}\")\n        return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_audio_dir = \"/kaggle/input/shl-intern-hiring-assessment/dataset/audios_train\"\ntrain_csv_path = \"/kaggle/input/shl-intern-hiring-assessment/dataset/train.csv\"\ntest_audio_dir = \"/kaggle/input/shl-intern-hiring-assessment/dataset/audios_test\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_transcriptions = {}\ntrain_df = pd.read_csv(train_csv_path)\n\nfor index, row in tqdm(train_df.iterrows(), total=len(train_df)):\n    audio_file_path = os.path.join(train_audio_dir, row['filename'])\n    transcription = transcribe_audio(audio_file_path)\n    if transcription is not None:\n        train_transcriptions[row['filename']] = transcription","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_texts = list(train_transcriptions.values())\nall_embeddings = get_bert_embeddings(all_texts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = []\nfor i, filename in enumerate(train_transcriptions.keys()):\n    if all_embeddings is not None:\n        embedding = all_embeddings[i]\n        train_data.append((embedding, train_df[train_df['filename'] == filename]['label'].values[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embeddings = torch.stack([item[0] for item in train_data]).to(\"cuda\")\nlabels = torch.tensor([item[1] for item in train_data]).float().unsqueeze(1).to(\"cuda\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = TensorDataset(embeddings, labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.Adam(mlp_head.parameters(), lr=1e-3)\ncriterion = torch.nn.MSELoss()\n\nepochs = 10\nfor epoch in range(epochs):\n    train_loss = 0.0\n    val_loss = 0.0\n\n    # Training\n    mlp_head.train()\n    for embeddings_batch, labels_batch in train_dataloader:\n        optimizer.zero_grad()\n        outputs = mlp_head(embeddings_batch) * 5 # scale output to 0-5\n        loss = criterion(outputs, labels_batch)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    train_loss /= len(train_dataloader)\n\n    # Validation\n    mlp_head.eval()\n    with torch.no_grad():\n        for embeddings_batch, labels_batch in val_dataloader:\n            outputs = mlp_head(embeddings_batch) * 5 # scale output to 0-5\n            loss = criterion(outputs, labels_batch)\n            val_loss += loss.item()\n    val_loss /= len(val_dataloader)\n\n    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = []\ntest_filenames = []\n\nfor filename in os.listdir(test_audio_dir):\n    if filename.endswith(\".wav\"):\n        audio_file_path = os.path.join(test_audio_dir, filename)\n        transcription = transcribe_audio(audio_file_path)\n        if transcription is not None:\n            embeddings = get_bert_embeddings(transcription)\n            if embeddings is not None:\n                test_data.append((filename, embeddings))\n                test_filenames.append(filename)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_predictions = []\nwith torch.no_grad():\n    for filename, embeddings in test_data:\n        outputs = mlp_head(embeddings) * 5\n        test_predictions.append(outputs.item())\n\ntest_predictions_clipped = np.clip(test_predictions, 0, 5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_df = pd.DataFrame({'filename': test_filenames, 'label': test_predictions_clipped})\noutput_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}