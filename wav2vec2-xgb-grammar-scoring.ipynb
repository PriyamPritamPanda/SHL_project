{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":97919,"databundleVersionId":11694977,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Import necessary libraries\nimport torch\nimport torchaudio\nimport torchaudio.transforms as T\nfrom transformers import Wav2Vec2Model, Wav2Vec2Processor\nimport os\nimport numpy as np\nfrom tqdm import tqdm_notebook as tqdm\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport optuna\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:01:26.339740Z","iopub.execute_input":"2025-04-05T19:01:26.340027Z","iopub.status.idle":"2025-04-05T19:01:26.344408Z","shell.execute_reply.started":"2025-04-05T19:01:26.340005Z","shell.execute_reply":"2025-04-05T19:01:26.343585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_wav2vec2_embeddings(audio_file_path, processor, model, device):\n    \"\"\"\n    Generates Wav2Vec2 embeddings for a given audio file.\n    \"\"\"\n    try:\n        waveform, sample_rate = torchaudio.load(audio_file_path)\n\n        # Convert to mono if stereo\n        if waveform.shape[0] == 2:\n            waveform = torch.mean(waveform, dim=0, keepdim=True)  # Average channels\n\n        waveform = torch.squeeze(waveform)  # Remove extra dimensions\n        inputs = processor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n        inputs = inputs.to(device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n        return embeddings\n    except Exception as e:\n        print(f\"Error processing {audio_file_path}: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:43:44.678608Z","iopub.execute_input":"2025-04-05T18:43:44.679133Z","iopub.status.idle":"2025-04-05T18:43:44.684898Z","shell.execute_reply.started":"2025-04-05T18:43:44.679107Z","shell.execute_reply":"2025-04-05T18:43:44.683951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_audio_files(audio_dir, processor, model, device):\n    \"\"\"\n    Processes all audio files in a directory and returns their embeddings.\n\n    Args:\n        audio_dir (str): Path to the directory containing audio files.\n        processor (Wav2Vec2Processor): Wav2Vec2 processor.\n        model (Wav2Vec2Model): Wav2Vec2 model.\n        device (torch.device): The device to run the model on (CPU or GPU).\n\n    Returns:\n        dict: A dictionary mapping audio file names to their embedding vectors.\n    \"\"\"\n    embeddings = {}\n    for filename in tqdm(os.listdir(audio_dir)):\n        if filename.endswith(\".wav\"):\n            audio_file_path = os.path.join(audio_dir, filename)\n            embedding = get_wav2vec2_embeddings(audio_file_path, processor, model, device)\n            if embedding is not None:\n                embeddings[filename] = embedding\n    return embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:43:44.686391Z","iopub.execute_input":"2025-04-05T18:43:44.686934Z","iopub.status.idle":"2025-04-05T18:43:44.718432Z","shell.execute_reply.started":"2025-04-05T18:43:44.686899Z","shell.execute_reply":"2025-04-05T18:43:44.717449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = \"facebook/wav2vec2-base-960h\"\nprocessor = Wav2Vec2Processor.from_pretrained(model_name)\nmodel = Wav2Vec2Model.from_pretrained(model_name)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ntrain_audio_dir = \"/kaggle/input/shl-intern-hiring-assessment/dataset/audios_train\"\ntrain_embeddings = process_audio_files(train_audio_dir, processor, model, device)\nif train_embeddings:\n    first_train_file = list(train_embeddings.keys())[0]\n    print(f\"Shape of embedding for {first_train_file}: {train_embeddings[first_train_file].shape}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:43:44.719498Z","iopub.execute_input":"2025-04-05T18:43:44.719844Z","iopub.status.idle":"2025-04-05T18:46:39.505035Z","shell.execute_reply.started":"2025-04-05T18:43:44.719811Z","shell.execute_reply":"2025-04-05T18:46:39.504038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_csv_path = \"/kaggle/input/shl-intern-hiring-assessment/dataset/train.csv\"\ntrain_df = pd.read_csv(train_csv_path)\ntrain_labels = {}\nfor index, row in train_df.iterrows():\n    train_labels[row['filename']] = row['label']\n\nfeatures = []\ntargets = []\nfor filename, embedding in train_embeddings.items():\n    if filename in train_labels:\n        features.append(embedding)\n        targets.append(train_labels[filename])\nfeatures = np.array(features)\ntargets = np.array(targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:46:39.506205Z","iopub.execute_input":"2025-04-05T18:46:39.506455Z","iopub.status.idle":"2025-04-05T18:46:39.543085Z","shell.execute_reply.started":"2025-04-05T18:46:39.506433Z","shell.execute_reply":"2025-04-05T18:46:39.542421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(features, targets, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:46:58.511215Z","iopub.execute_input":"2025-04-05T18:46:58.511541Z","iopub.status.idle":"2025-04-05T18:46:58.518423Z","shell.execute_reply.started":"2025-04-05T18:46:58.511515Z","shell.execute_reply":"2025-04-05T18:46:58.517698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective(trial):\n    \"\"\"Objective function for Optuna.\"\"\"\n    param = {\n        'objective': 'reg:squarederror',\n        'random_state': 42,\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'gamma': trial.suggest_float('gamma', 0, 1),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),\n    }\n\n    xgb_model = xgb.XGBRegressor(**param)\n    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False, early_stopping_rounds=10)\n\n    y_pred = xgb_model.predict(X_val)\n    y_pred_transformed = np.clip(y_pred, 0, 5)\n\n    rmse = np.sqrt(mean_squared_error(y_val, y_pred_transformed))\n    return rmse","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:59:46.824748Z","iopub.execute_input":"2025-04-05T18:59:46.825082Z","iopub.status.idle":"2025-04-05T18:59:46.830970Z","shell.execute_reply.started":"2025-04-05T18:59:46.825056Z","shell.execute_reply":"2025-04-05T18:59:46.830074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)  # Adjust n_trials as needed\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:59:47.496986Z","iopub.execute_input":"2025-04-05T18:59:47.497305Z","iopub.status.idle":"2025-04-05T19:01:25.983484Z","shell.execute_reply.started":"2025-04-05T18:59:47.497276Z","shell.execute_reply":"2025-04-05T19:01:25.982729Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_params = trial.params\nbest_params['objective'] = 'reg:squarederror'\nbest_params['random_state'] = 42\n\nfinal_xgb_model = xgb.XGBRegressor(**best_params)\nfinal_xgb_model.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:01:56.942442Z","iopub.execute_input":"2025-04-05T19:01:56.942777Z","iopub.status.idle":"2025-04-05T19:02:00.283243Z","shell.execute_reply.started":"2025-04-05T19:01:56.942752Z","shell.execute_reply":"2025-04-05T19:02:00.282342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = final_xgb_model.predict(X_val)\ny_pred_clipped = np.clip(y_pred, 0, 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:03:04.662211Z","iopub.execute_input":"2025-04-05T19:03:04.662551Z","iopub.status.idle":"2025-04-05T19:03:04.667958Z","shell.execute_reply.started":"2025-04-05T19:03:04.662521Z","shell.execute_reply":"2025-04-05T19:03:04.667003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rmse = np.sqrt(mean_squared_error(y_val, y_pred_clipped))\nprint(f\"Root Mean Squared Error (Clipped): {rmse}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:03:07.987470Z","iopub.execute_input":"2025-04-05T19:03:07.987923Z","iopub.status.idle":"2025-04-05T19:03:07.993314Z","shell.execute_reply.started":"2025-04-05T19:03:07.987889Z","shell.execute_reply":"2025-04-05T19:03:07.992497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_audio_dir = \"/kaggle/input/shl-intern-hiring-assessment/dataset/audios_test\"\ntest_embeddings = process_audio_files(test_audio_dir, processor, model, device)\ntest_features = []\ntest_filenames = []\nfor filename, embedding in test_embeddings.items():\n    test_features.append(embedding)\n    test_filenames.append(filename)\ntest_predictions = final_xgb_model.predict(np.array(test_features))\ntest_predictions_clipped = np.clip(test_predictions, 0, 5)  # Clip test predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:06:28.556095Z","iopub.execute_input":"2025-04-05T19:06:28.556460Z","iopub.status.idle":"2025-04-05T19:07:37.449288Z","shell.execute_reply.started":"2025-04-05T19:06:28.556433Z","shell.execute_reply":"2025-04-05T19:07:37.448545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_df = pd.DataFrame({'filename': test_filenames, 'label': test_predictions_clipped})\noutput_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}